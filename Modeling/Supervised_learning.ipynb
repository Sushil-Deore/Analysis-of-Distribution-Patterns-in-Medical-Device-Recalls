{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_curve, auc\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setting the maximum number of displayed columns to 'None' (unlimited)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Setting the maximum number of displayed rows to 'None' (unlimited)\n",
    "pd.set_option(\"display.max_row\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading clean_openFDA file\n",
    "df_openFDA = pd.read_csv('clean_openFDA.csv')\n",
    "\n",
    "df_openFDA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dataframe structure\n",
    "df_openFDA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking number of columns and rows\n",
    "df_openFDA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping column \"Unnamed: 0\"\n",
    "df_openFDA = df_openFDA.drop(['Unnamed: 0', 'state', 'cleaned_reason_for_recall', 'cleaned_product_description', 'cleaned_action'], axis=1)\n",
    "\n",
    "# Checking dataframe again\n",
    "df_openFDA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "- Obejct to numerical columns\n",
    "- OneHotEncoding & labelEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_openFDA.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the values to be removed\n",
    "values_to_remove = ['U', 'N', 'f']\n",
    "\n",
    "# Filter the DataFrame to exclude rows with these values\n",
    "df_openFDA = df_openFDA[~df_openFDA['openfda.device_class'].isin(values_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_openFDA['openfda.device_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_openFDA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets decide and perfom OneHotEncoding on columns\n",
    "\n",
    "RCD_dum = pd.get_dummies(df_openFDA['root_cause_description'], prefix= 'RCD', dtype=int)\n",
    "Distribution_dum = pd.get_dummies(df_openFDA['distribution'], prefix= 'distribution', dtype=int)\n",
    "EMP_dum = pd.get_dummies(df_openFDA['event_month_posted'], prefix= 'EMP', dtype=int)\n",
    "EYP_dum = pd.get_dummies(df_openFDA['event_year_posted'], prefix= 'EYP', dtype=int)\n",
    "\n",
    "DOW_dum = pd.get_dummies(df_openFDA['day_of_week_posted'], prefix= 'EYP', dtype=int)\n",
    "EMT_dum = pd.get_dummies(df_openFDA['event_month_terminated'], prefix= 'EYP', dtype=int)\n",
    "EYT_dum = pd.get_dummies(df_openFDA['event_year_terminated'], prefix= 'EYP', dtype=int)\n",
    "DOWT_dum = pd.get_dummies(df_openFDA['day_of_week_terminated'], prefix= 'EYP', dtype=int)\n",
    "\n",
    "\n",
    "df_openFDA = pd.concat([df_openFDA, \n",
    "                        RCD_dum, \n",
    "                        Distribution_dum, \n",
    "                        EMP_dum, \n",
    "                        EYP_dum, \n",
    "                        DOW_dum, \n",
    "                        EMT_dum, \n",
    "                        EYT_dum, \n",
    "                        DOWT_dum], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_openFDA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_openFDA = df_openFDA.drop(['root_cause_description',\n",
    "                             'event_month_posted', \n",
    "                             'event_year_posted', \n",
    "                             'day_of_week_posted', \n",
    "                             'event_month_terminated', \n",
    "                             'event_year_terminated', \n",
    "                             'day_of_week_terminated',\n",
    "                             'distribution'], axis=1)\n",
    "\n",
    "df_openFDA.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = df_openFDA.drop('openfda.device_class', axis=1)\n",
    "y = df_openFDA['openfda.device_class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train shape {X_train.shape}')\n",
    "print(f'y_train shape {y_train.shape}')\n",
    "print(f'X_test shape {X_test.shape}')\n",
    "print(f'y_test shape {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "multi_NB = MultinomialNB()\n",
    "\n",
    "multi_NB.fit(X_train, y_train, sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = multi_NB.predict(X_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = multi_NB.predict(X_train)\n",
    "\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "unique_values, counts = np.unique(y_pred_train, return_counts=True)\n",
    "\n",
    "# Create a dictionary with the unique values and their counts\n",
    "value_counts = dict(zip(unique_values, counts))\n",
    "\n",
    "# Print the value counts\n",
    "for value, count in value_counts.items():\n",
    "    print(f'Value: {value}, Count: {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each class\n",
    "class_counts = df_openFDA['openfda.device_class'].value_counts()\n",
    "\n",
    "# Extract class labels and their respective counts\n",
    "class_labels = class_counts.index\n",
    "counts = class_counts.values\n",
    "\n",
    "# Define a color palette for the bars (you can customize the colors)\n",
    "colors = ['skyblue', 'salmon', 'lightgreen']\n",
    "\n",
    "# Create a bar chart to visualize the class distribution with colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(class_labels, counts, color=colors)\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution')\n",
    "plt.xticks(rotation=0)  # Rotate x-axis labels for better readability\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(multi_NB.score(X_train, y_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(multi_NB.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Confusion Matrix and slice it into four pieces\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define the number of classes (replace this with your number of classes)\n",
    "num_classes = 3\n",
    "\n",
    "# Initialize dictionaries to store TP, TN, FP, FN for each class\n",
    "class_1 = {}\n",
    "class_2 = {}\n",
    "class_3 = {}\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "for class_idx in range(num_classes):\n",
    "    # Extract the relevant row and column for the current class\n",
    "    TP_value = confusion_matrix[class_idx, class_idx]\n",
    "    FP_value = np.sum(confusion_matrix[:, class_idx]) - TP_value\n",
    "    FN_value = np.sum(confusion_matrix[class_idx, :]) - TP_value\n",
    "    TN_value = np.sum(confusion_matrix) - (TP_value + FP_value + FN_value)\n",
    "\n",
    "    # Append the calculated values to the respective class dictionaries\n",
    "    if class_idx == 0:\n",
    "        class_1[\"TP\"] = TP_value\n",
    "        class_1[\"TN\"] = TN_value\n",
    "        class_1[\"FP\"] = FP_value\n",
    "        class_1[\"FN\"] = FN_value\n",
    "    elif class_idx == 1:\n",
    "        class_2[\"TP\"] = TP_value\n",
    "        class_2[\"TN\"] = TN_value\n",
    "        class_2[\"FP\"] = FP_value\n",
    "        class_2[\"FN\"] = FN_value\n",
    "    elif class_idx == 2:\n",
    "        class_3[\"TP\"] = TP_value\n",
    "        class_3[\"TN\"] = TN_value\n",
    "        class_3[\"FP\"] = FP_value\n",
    "        class_3[\"FN\"] = FN_value\n",
    "\n",
    "# Now, you have dictionaries class_1, class_2, and class_3 with key-value pairs\n",
    "print(\"Class 1:\")\n",
    "print(class_1)\n",
    "\n",
    "print(\"Class 2:\")\n",
    "print(class_2)\n",
    "\n",
    "print(\"Class 3:\")\n",
    "print(class_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_1_cm = class_1['TP'] + class_1['TN'] + class_1['FP'] + class_1['FN']\n",
    "class_2_cm = class_2['TP'] + class_2['TN'] + class_2['FP'] + class_2['FN']\n",
    "class_3_cm =  class_3['TP'] + class_3['TN'] + class_3['FP'] + class_3['FN']\n",
    "\n",
    "# Calculate classification accuracy\n",
    "accuracy = (class_1['TP'] + class_2['TP'] + class_3['TP']) / (class_1_cm + class_1_cm + class_1_cm)\n",
    "\n",
    "# Calculate classification error\n",
    "error = 1 - accuracy\n",
    "\n",
    "# Calculate precision, recall, true positive rate, false positive rate, and specificity for each class\n",
    "def calculate_metrics(class_dict):\n",
    "    precision = class_dict['TP'] / (class_dict['TP'] + class_dict['FP'])\n",
    "    recall = class_dict['TP'] / (class_dict['TP'] + class_dict['FN'])\n",
    "    true_positive_rate = recall\n",
    "    false_positive_rate = class_dict['FP'] / (class_dict['TN'] + class_dict['FP'])\n",
    "    specificity = class_dict['TN'] / (class_dict['TN'] + class_dict['FP'])\n",
    "    return precision, recall, true_positive_rate, false_positive_rate, specificity\n",
    "\n",
    "class_1_metrics = calculate_metrics(class_1)\n",
    "class_2_metrics = calculate_metrics(class_2)\n",
    "class_3_metrics = calculate_metrics(class_3)\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(\"Classification Accuracy:\", accuracy)\n",
    "print()\n",
    "print(\"Classification Error:\", error)\n",
    "print()\n",
    "print(f\"Class 1 Metrics \\n Precision = {class_1_metrics[0]} \\n Recall = {class_1_metrics[1]} \\n True Positive Rate = {class_1_metrics[2]} \\n False Positive Rate = {class_1_metrics[3]} \\n Specificity = {class_1_metrics[4]} \\n\")\n",
    "print()\n",
    "print(f\"Class 2 Metrics \\n Precision = {class_2_metrics[0]} \\n Recall = {class_2_metrics[1]} \\n True Positive Rate = {class_2_metrics[2]} \\n False Positive Rate = {class_2_metrics[3]} \\n Specificity = {class_2_metrics[4]} \\n\")\n",
    "print()\n",
    "print(f\"Class 3 Metrics \\n Precision = {class_3_metrics[0]} \\n Recall = {class_3_metrics[1]} \\n True Positive Rate = {class_3_metrics[2]} \\n False Positive Rate = {class_3_metrics[3]} \\n Specificity = {class_3_metrics[4]} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = df_openFDA.drop('openfda.device_class', axis=1)\n",
    "y = df_openFDA['openfda.device_class']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train shape {X_train.shape}')\n",
    "print(f'y_train shape {y_train.shape}')\n",
    "print(f'X_test shape {X_test.shape}')\n",
    "print(f'y_test shape {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pydotplus\n",
    "#%pip install python-graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required packages for visualization\n",
    "from IPython.display import Image  \n",
    "from six import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting tree with max_depth=3\n",
    "dot_data = StringIO()  \n",
    "\n",
    "export_graphviz(dt, out_file=dot_data, filled=True, rounded=False,\n",
    "                feature_names=X.columns, \n",
    "                class_names=['1', '2', '3'])\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = dt.predict(X_train)\n",
    "y_test_pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print(accuracy_score(y_train, y_train_pred))\n",
    "confusion_matrix(y_train, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_test_pred))\n",
    "confusion_matrix(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dt_graph(dt_classifier):\n",
    "    dot_data = StringIO()  \n",
    "\n",
    "    export_graphviz(dt_classifier, out_file=dot_data, filled=True, rounded=False,\n",
    "                feature_names=X.columns, \n",
    "                class_names=['1', '2', '3'])\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dt_classifier):\n",
    "    y_train_pred = dt_classifier.predict(X_train)\n",
    "    y_test_pred = dt_classifier.predict(X_test)\n",
    "    \n",
    "    print(\"Train Set Performance\")\n",
    "    print(accuracy_score(y_train, y_train_pred))\n",
    "    print(confusion_matrix(y_train, y_train_pred))\n",
    "    print(\"--\"*25)\n",
    "    print(\"Test Set Performance\")\n",
    "    print(accuracy_score(y_test, y_test_pred))\n",
    "    print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gph = get_dt_graph(dt)\n",
    "Image(gph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_depth = DecisionTreeClassifier(max_depth = 4, random_state = 42)\n",
    "dt_depth.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gph = get_dt_graph(dt_depth)\n",
    "Image(gph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dt_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_min_split = DecisionTreeClassifier(min_samples_split =10, random_state = 42)\n",
    "dt_min_split.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gph = get_dt_graph(dt_min_split)\n",
    "Image(gph.create_png(), width=1000, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dt_min_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_min_leaf = DecisionTreeClassifier(min_samples_leaf = 5, random_state = 42)\n",
    "dt_min_leaf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gph = get_dt_graph(dt_min_leaf)\n",
    "Image(gph.create_png(), width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dt_min_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_min_leaf_entropy = DecisionTreeClassifier(min_samples_leaf = 20, random_state = 42, criterion = \"entropy\")\n",
    "dt_min_leaf_entropy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gph = get_dt_graph(dt_min_leaf_entropy)\n",
    "Image(gph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dt_min_leaf_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\" : [2,3,5],\n",
    "    \"min_samples_leaf\" : [5,8,12,15],\n",
    "    \"criterion\" : ['gini', 'entropy']    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid_search = GridSearchCV(estimator = dt,\n",
    "            param_grid = params,\n",
    "            cv = 4,\n",
    "            n_jobs = -1,\n",
    "            verbose = 1,\n",
    "            scoring = 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df.nlargest(5, 'mean_test_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_best = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(dt_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gph=get_dt_graph(dt_best)\n",
    "Image(gph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = dt_best.predict(X_train)\n",
    "y_test_pred = dt_best.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train Set Performance\")\n",
    "print(accuracy_score(y_train, y_train_pred))\n",
    "print(confusion_matrix(y_train, y_train_pred))\n",
    "print(\"--\"*25)\n",
    "print(\"Test Set Performance\")\n",
    "print(accuracy_score(y_test, y_test_pred))\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Confusion Matrix and slice it into four pieces\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define the number of classes (replace this with your number of classes)\n",
    "num_classes = 3\n",
    "\n",
    "# Initialize dictionaries to store TP, TN, FP, FN for each class\n",
    "class_1 = {}\n",
    "class_2 = {}\n",
    "class_3 = {}\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "for class_idx in range(num_classes):\n",
    "    # Extract the relevant row and column for the current class\n",
    "    TP_value = confusion_matrix[class_idx, class_idx]\n",
    "    FP_value = np.sum(confusion_matrix[:, class_idx]) - TP_value\n",
    "    FN_value = np.sum(confusion_matrix[class_idx, :]) - TP_value\n",
    "    TN_value = np.sum(confusion_matrix) - (TP_value + FP_value + FN_value)\n",
    "\n",
    "    # Append the calculated values to the respective class dictionaries\n",
    "    if class_idx == 0:\n",
    "        class_1[\"TP\"] = TP_value\n",
    "        class_1[\"TN\"] = TN_value\n",
    "        class_1[\"FP\"] = FP_value\n",
    "        class_1[\"FN\"] = FN_value\n",
    "    elif class_idx == 1:\n",
    "        class_2[\"TP\"] = TP_value\n",
    "        class_2[\"TN\"] = TN_value\n",
    "        class_2[\"FP\"] = FP_value\n",
    "        class_2[\"FN\"] = FN_value\n",
    "    elif class_idx == 2:\n",
    "        class_3[\"TP\"] = TP_value\n",
    "        class_3[\"TN\"] = TN_value\n",
    "        class_3[\"FP\"] = FP_value\n",
    "        class_3[\"FN\"] = FN_value\n",
    "\n",
    "# Now, you have dictionaries class_1, class_2, and class_3 with key-value pairs\n",
    "print(\"Class 1:\")\n",
    "print(class_1)\n",
    "\n",
    "print(\"Class 2:\")\n",
    "print(class_2)\n",
    "\n",
    "print(\"Class 3:\")\n",
    "print(class_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training set score: {:.4f}'.format(dt_best.score(X_train, y_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(dt_best.score(X_test, y_test)))\n",
    "\n",
    "# Calculate classification accuracy\n",
    "accuracy = (class_1['TP'] + class_2['TP'] + class_3['TP']) / (class_1['TP'] + class_1['TN'] + class_1['FP'] + class_1['FN'] +\n",
    "                                                            class_2['TP'] + class_2['TN'] + class_2['FP'] + class_2['FN'] +\n",
    "                                                            class_3['TP'] + class_3['TN'] + class_3['FP'] + class_3['FN'])\n",
    "\n",
    "# Calculate classification error\n",
    "error = 1 - accuracy\n",
    "\n",
    "# Calculate precision, recall, true positive rate, false positive rate, and specificity for each class\n",
    "def calculate_metrics(class_dict):\n",
    "    precision = class_dict['TP'] / (class_dict['TP'] + class_dict['FP'])\n",
    "    recall = class_dict['TP'] / (class_dict['TP'] + class_dict['FN'])\n",
    "    true_positive_rate = recall\n",
    "    false_positive_rate = class_dict['FP'] / (class_dict['TN'] + class_dict['FP'])\n",
    "    specificity = class_dict['TN'] / (class_dict['TN'] + class_dict['FP'])\n",
    "    return precision, recall, true_positive_rate, false_positive_rate, specificity\n",
    "\n",
    "class_1_metrics = calculate_metrics(class_1)\n",
    "class_2_metrics = calculate_metrics(class_2)\n",
    "class_3_metrics = calculate_metrics(class_3)\n",
    "\n",
    "# Print the calculated metrics\n",
    "print(\"Classification Accuracy:\", accuracy)\n",
    "print()\n",
    "print(\"Classification Error:\", error)\n",
    "print()\n",
    "print(f\"Class 1 Metrics \\n Precision = {class_1_metrics[0]} \\n Recall = {class_1_metrics[1]} \\n True Positive Rate = {class_1_metrics[2]} \\n False Positive Rate = {class_1_metrics[3]} \\n Specificity = {class_1_metrics[4]} \\n\")\n",
    "print()\n",
    "print(f\"Class 2 Metrics \\n Precision = {class_2_metrics[0]} \\n Recall = {class_2_metrics[1]} \\n True Positive Rate = {class_2_metrics[2]} \\n False Positive Rate = {class_2_metrics[3]} \\n Specificity = {class_2_metrics[4]} \\n\")\n",
    "print()\n",
    "print(f\"Class 3 Metrics \\n Precision = {class_3_metrics[0]} \\n Recall = {class_3_metrics[1]} \\n True Positive Rate = {class_3_metrics[2]} \\n False Positive Rate = {class_3_metrics[3]} \\n Specificity = {class_3_metrics[4]} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 2D point\n",
    "point_2d = np.array([2, 3])\n",
    "\n",
    "# Polynomial kernel transformation\n",
    "r = 1\n",
    "d = 2\n",
    "point_transformed = np.array([1, point_2d[0], point_2d[1], point_2d[0]**2, point_2d[0]*point_2d[1], point_2d[1]**2])\n",
    "\n",
    "# Plot original and transformed points\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Original 2D point\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.scatter(point_2d[0], point_2d[1], c='blue', marker='o', label='Original 2D Point')\n",
    "ax1.set_title('Original 2D Point')\n",
    "ax1.set_xlabel('X1')\n",
    "ax1.set_ylabel('X2')\n",
    "ax1.legend()\n",
    "\n",
    "# Transformed 6D point\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "ax2.scatter(point_transformed[1], point_transformed[4], point_transformed[2],\n",
    "            c='red', marker='o', label='Transformed 6D Point')\n",
    "ax2.set_title('Transformed 6D Point')\n",
    "ax2.set_xlabel('X1')\n",
    "ax2.set_ylabel('X1 * X2')\n",
    "ax2.set_zlabel('X2')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a random subset of 1000 rows (adjust if needed)\n",
    "subset_size = 1000\n",
    "data_subset = df_openFDA.sample(n=subset_size, random_state=42)\n",
    "\n",
    "# Extract features and labels from the subset\n",
    "X_subset = data_subset.drop('openfda.device_class', axis=1)\n",
    "y_subset = data_subset['openfda.device_class']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_subset, y_subset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (important for SVM)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SVM modeling with different kernels and costs\n",
    "kernels = ['linear', 'poly', 'rbf']\n",
    "costs = [0.1, 1, 10]\n",
    "\n",
    "# Create subplots for confusion matrices\n",
    "fig, axes = plt.subplots(nrows=len(kernels), ncols=len(costs), figsize=(15, 12))\n",
    "\n",
    "for i, kernel in enumerate(kernels):\n",
    "    for j, cost in enumerate(costs):\n",
    "        model = SVC(kernel=kernel, C=cost, probability=True)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Predictions\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "        # Evaluate and print results\n",
    "        print(f\"Kernel: {kernel}, Cost: {cost}\")\n",
    "        print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        # Visualization: Confusion Matrix Heatmap\n",
    "        classes = sorted(y_test.unique())  # Ensure the order of classes\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=classes)\n",
    "        \n",
    "        # Plot confusion matrix heatmap\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes, ax=axes[i, j])\n",
    "        axes[i, j].set_title(f\"Kernel: {kernel}, Cost: {cost}\")\n",
    "        axes[i, j].set_xlabel(\"Predicted Label\")\n",
    "        axes[i, j].set_ylabel(\"True Label\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print accuracies for all combinations\n",
    "print(\"\\nAccuracies:\")\n",
    "for i, kernel in enumerate(kernels):\n",
    "    for j, cost in enumerate(costs):\n",
    "        model = SVC(kernel=kernel, C=cost, probability=True)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "        print(f\"Kernel: {kernel}, Cost: {cost}, Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
